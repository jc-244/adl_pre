{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch `nn.MultiheadAttention` (Reading-Only, Core Excerpt)\n",
        "\n",
        "This notebook is for **reading** only (not meant to run).\n",
        "\n",
        "Source (exact commit/lines):\n",
        "- https://github.com/pytorch/pytorch/blob/5a48148c1ab83c1e3779283d904ba5744bbe8eb3/torch/nn/modules/activation.py#L1091-L1568\n",
        "\n",
        "Local snapshot used here: `dl_pre/1.py`\n",
        "\n",
        "Rules for the code excerpt below:\n",
        "- Only deletions from `1.py` (no edits to any kept line)\n",
        "- Deleted blocks are replaced with `# ...`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Docstring (verbatim from `1.py`)\n",
        "```text\n",
        "Allows the model to jointly attend to information from different representation subspaces.\n",
        "\n",
        "    This MultiheadAttention layer implements the original architecture described\n",
        "    in the `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_ paper. The\n",
        "    intent of this layer is as a reference implementation for foundational understanding\n",
        "    and thus it contains only limited features relative to newer architectures.\n",
        "    Given the fast pace of innovation in transformer-like architectures, we recommend\n",
        "    exploring this `tutorial <https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n",
        "    to build efficient layers from building blocks in core or using higher\n",
        "    level libraries from the `PyTorch Ecosystem <https://landscape.pytorch.org/>`_.\n",
        "\n",
        "    Multi-Head Attention is defined as:\n",
        "\n",
        "    .. math::\n",
        "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
        "\n",
        "    where :math:`\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\n",
        "\n",
        "    ``nn.MultiheadAttention`` will use the optimized implementations of\n",
        "    ``scaled_dot_product_attention()`` when possible.\n",
        "\n",
        "    In addition to support for the new ``scaled_dot_product_attention()``\n",
        "    function, for speeding up Inference, MHA will use\n",
        "    fastpath inference with support for Nested Tensors, iff:\n",
        "\n",
        "    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).\n",
        "    - inputs are batched (3D) with ``batch_first==True``\n",
        "    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``\n",
        "    - training is disabled (using ``.eval()``)\n",
        "    - ``add_bias_kv`` is ``False``\n",
        "    - ``add_zero_attn`` is ``False``\n",
        "    - ``kdim`` and ``vdim`` are equal to ``embed_dim``\n",
        "    - if a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ is passed, neither ``key_padding_mask``\n",
        "      nor ``attn_mask`` is passed\n",
        "    - autocast is disabled\n",
        "\n",
        "    If the optimized inference fastpath implementation is in use, a\n",
        "    `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be passed for\n",
        "    ``query``/``key``/``value`` to represent padding more efficiently than using a\n",
        "    padding mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_\n",
        "    will be returned, and an additional speedup proportional to the fraction of the input\n",
        "    that is padding can be expected.\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Total dimension of the model.\n",
        "        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\n",
        "            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\n",
        "        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\n",
        "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n",
        "        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n",
        "        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n",
        "            Default: ``False``.\n",
        "        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n",
        "        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n",
        "        batch_first: If ``True``, then the input and output tensors are provided\n",
        "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> # xdoctest: +SKIP\n",
        "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "\n",
        "    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:\n",
        "         https://arxiv.org/abs/2205.14135\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `forward(...)` Docstring (verbatim from `1.py`)\n",
        "```text\n",
        "        r\"\"\"Compute attention outputs using query, key, and value embeddings.\n",
        "\n",
        "            Supports optional parameters for padding, masks and attention weights.\n",
        "\n",
        "        Args:\n",
        "            query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``\n",
        "                or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,\n",
        "                :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.\n",
        "                Queries are compared against key-value pairs to produce the output.\n",
        "                See \"Attention Is All You Need\" for more details.\n",
        "            key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``\n",
        "                or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,\n",
        "                :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.\n",
        "                See \"Attention Is All You Need\" for more details.\n",
        "            value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when\n",
        "                ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source\n",
        "                sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.\n",
        "                See \"Attention Is All You Need\" for more details.\n",
        "            key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``\n",
        "                to ignore for the purpose of attention (i.e. treat as \"padding\"). For unbatched `query`, shape should be :math:`(S)`.\n",
        "                Binary and float masks are supported.\n",
        "                For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for\n",
        "                the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.\n",
        "            need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.\n",
        "                Set ``need_weights=False`` to use the optimized ``scaled_dot_product_attention``\n",
        "                and achieve the best performance for MHA.\n",
        "                Default: ``True``.\n",
        "            attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape\n",
        "                :math:`(L, S)` or :math:`(N\\cdot\\text{num\\_heads}, L, S)`, where :math:`N` is the batch size,\n",
        "                :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be\n",
        "                broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.\n",
        "                Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the\n",
        "                corresponding position is not allowed to attend. For a float mask, the mask values will be added to\n",
        "                the attention weight.\n",
        "                If both attn_mask and key_padding_mask are supplied, their types should match.\n",
        "            average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
        "                heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
        "                effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)\n",
        "            is_causal: If specified, applies a causal mask as attention mask.\n",
        "                Default: ``False``.\n",
        "                Warning:\n",
        "                ``is_causal`` provides a hint that ``attn_mask`` is the\n",
        "                causal mask. Providing incorrect hints can result in\n",
        "                incorrect execution, including forward and backward\n",
        "                compatibility.\n",
        "\n",
        "        Outputs:\n",
        "            - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,\n",
        "              :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,\n",
        "              where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the\n",
        "              embedding dimension ``embed_dim``.\n",
        "            - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,\n",
        "              returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or\n",
        "              :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and\n",
        "              :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
        "              head of shape :math:`(\\text{num\\_heads}, L, S)` when input is unbatched or :math:`(N, \\text{num\\_heads}, L, S)`.\n",
        "\n",
        "            .. note::\n",
        "                `batch_first` argument is ignored for unbatched inputs.\n",
        "        \"\"\"  # noqa: B950\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `merge_masks(...)` Docstring (verbatim from `1.py`)\n",
        "```text\n",
        "        r\"\"\"Determine mask type and combine masks if necessary.\n",
        "\n",
        "        If only one mask is provided, that mask\n",
        "        and the corresponding mask type will be returned. If both masks are provided, they will be both\n",
        "        expanded to shape ``(batch_size, num_heads, seq_len, seq_len)``, combined with logical ``or``\n",
        "        and mask type 2 will be returned\n",
        "        Args:\n",
        "            attn_mask: attention mask of shape ``(seq_len, seq_len)``, mask type 0\n",
        "            key_padding_mask: padding mask of shape ``(batch_size, seq_len)``, mask type 1\n",
        "            query: query embeddings of shape ``(batch_size, seq_len, embed_dim)``\n",
        "        Returns:\n",
        "            merged_mask: merged mask\n",
        "            mask_type: merged mask type (0, 1, or 2)\n",
        "        \"\"\"\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inline `# ...` Comments (verbatim from `1.py`)\n",
        "```text\n",
        "0172:         # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
        "0250:         \"\"\"  # noqa: B950\n",
        "0292:             # When lifting this restriction, don't forget to either\n",
        "0293:             # enforce that the dtypes all match or test cases where\n",
        "0294:             # they don't!\n",
        "0301:             # this case will fail anyway, but at least they'll get a useful error message.\n",
        "0338:             # We have to use list comprehensions below because TorchScript does not support\n",
        "0339:             # generator expressions.\n",
        "0389:             # make sure that the transpose op does not affect the \"is\" property\n",
        "0484:             # In this branch query can't be a nested tensor, so it has a shape\n",
        "0489:             # Always expands attn_mask to 4D\n",
        "0492:             else:  # attn_mask.dim() == 2:\n",
        "0506:         # no attn_mask and no key_padding_mask, returns None, None\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiheadAttention(Module):\n",
        "    # ...\n",
        "    __constants__ = [\"batch_first\"]\n",
        "    bias_k: torch.Tensor | None\n",
        "    bias_v: torch.Tensor | None\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=True,\n",
        "        add_bias_kv=False,\n",
        "        add_zero_attn=False,\n",
        "        kdim=None,\n",
        "        vdim=None,\n",
        "        batch_first=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        if embed_dim <= 0 or num_heads <= 0:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim and num_heads must be greater than 0,\"\n",
        "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
        "            )\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        if self.head_dim * num_heads != self.embed_dim:\n",
        "            raise AssertionError(\"embed_dim must be divisible by num_heads\")\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            # ...\n",
        "        else:\n",
        "            self.in_proj_weight = Parameter(\n",
        "                torch.empty((3 * embed_dim, embed_dim), **factory_kwargs)\n",
        "            )\n",
        "            self.register_parameter(\"q_proj_weight\", None)\n",
        "            self.register_parameter(\"k_proj_weight\", None)\n",
        "            self.register_parameter(\"v_proj_weight\", None)\n",
        "\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter(\"in_proj_bias\", None)\n",
        "        self.out_proj = NonDynamicallyQuantizableLinear(\n",
        "            embed_dim, embed_dim, bias=bias, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self) -> None:\n",
        "        if self._qkv_same_embed_dim:\n",
        "            xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            # ...\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.0)\n",
        "            constant_(self.out_proj.bias, 0.0)\n",
        "        if self.bias_k is not None:\n",
        "            xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            xavier_normal_(self.bias_v)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: Tensor,\n",
        "        key: Tensor,\n",
        "        value: Tensor,\n",
        "        key_padding_mask: Tensor | None = None,\n",
        "        need_weights: bool = True,\n",
        "        attn_mask: Tensor | None = None,\n",
        "        average_attn_weights: bool = True,\n",
        "        is_causal: bool = False,\n",
        "    ) -> tuple[Tensor, Tensor | None]:\n",
        "        # ...\n",
        "        is_batched = query.dim() == 3\n",
        "\n",
        "        if self.batch_first and is_batched:\n",
        "            # ...\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                self.in_proj_weight,\n",
        "                self.in_proj_bias,\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=attn_mask,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight,\n",
        "                k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight,\n",
        "                average_attn_weights=average_attn_weights,\n",
        "                is_causal=is_causal,\n",
        "            )\n",
        "        else:\n",
        "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                self.in_proj_weight,\n",
        "                self.in_proj_bias,\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=attn_mask,\n",
        "                average_attn_weights=average_attn_weights,\n",
        "                is_causal=is_causal,\n",
        "            )\n",
        "        if self.batch_first and is_batched:\n",
        "            return attn_output.transpose(1, 0), attn_output_weights\n",
        "        else:\n",
        "            return attn_output, attn_output_weights"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
