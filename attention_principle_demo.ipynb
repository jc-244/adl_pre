{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31bf0f59",
      "metadata": {},
      "source": [
        "# Attention Principle (Course Pre Demo)\n",
        "\n",
        "Goal: show attention as learnable **alignment / addressing** via heatmaps (copy = diagonal, reverse = anti-diagonal, pointer = sharp peak).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42979c51",
      "metadata": {},
      "source": [
        "## Setup\n",
        "This notebook calls PyTorch's `torch.nn.MultiheadAttention` (no custom attention implementation here).\n",
        "For reading PyTorch's implementation (trimmed by deletion only), see `pytorch_mha_reading.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b38220b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "\n",
        "set_seed(0)\n",
        "torch.set_num_threads(min(4, os.cpu_count() or 1))\n",
        "device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "befb3c2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CrossAttnSeqClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, max_in_len, max_out_len):\n",
        "        super().__init__()\n",
        "        self.vocab_size, self.max_in_len, self.max_out_len = vocab_size, max_in_len, max_out_len\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.in_pos_emb = nn.Embedding(max_in_len, d_model)\n",
        "        self.out_pos_emb = nn.Embedding(max_out_len, d_model)\n",
        "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=0.0, batch_first=True)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, key_tokens, value_tokens, out_len, query_token=None):\n",
        "        B, Tk = key_tokens.shape\n",
        "        if Tk > self.max_in_len or out_len > self.max_out_len:\n",
        "            raise ValueError('sequence too long for positional embeddings')\n",
        "        in_pos = torch.arange(Tk, device=key_tokens.device)\n",
        "        out_pos = torch.arange(out_len, device=key_tokens.device)\n",
        "        K = self.tok_emb(key_tokens) + self.in_pos_emb(in_pos)[None, :, :]\n",
        "        V = self.tok_emb(value_tokens) + self.in_pos_emb(in_pos)[None, :, :]\n",
        "        Q = self.out_pos_emb(out_pos)[None, :, :].expand(B, -1, -1)\n",
        "        if query_token is not None:\n",
        "            Q = Q + self.tok_emb(query_token)[:, None, :]\n",
        "        ctx, attn_w = self.attn(Q, K, V, need_weights=True, average_attn_weights=False)\n",
        "        return self.out(ctx), attn_w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68eb82b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_batch(task, B, L, V, M=12, K=12, Val=12):\n",
        "    if task != 'ptr':\n",
        "        x = torch.randint(1, V, (B, L), dtype=torch.long)\n",
        "        y = x if task == 'copy' else x.flip(1)\n",
        "        return x, x, y, None, L\n",
        "    keys = torch.randint(1, K + 1, (B, M), dtype=torch.long)\n",
        "    vals = torch.randint(K + 1, K + Val + 1, (B, M), dtype=torch.long)\n",
        "    idx = torch.randint(0, M, (B,), dtype=torch.long)\n",
        "    q = keys[torch.arange(B), idx]\n",
        "    y = vals[torch.arange(B), idx]\n",
        "    return keys, vals, y, q, 1\n",
        "\n",
        "def train(task, steps=250, L=16, V=32, M=12, K=12, Val=12, d_model=32, heads=4, lr=3e-3):\n",
        "    vocab = V if task != 'ptr' else 1 + K + Val\n",
        "    max_in, max_out = (L, L) if task != 'ptr' else (M, 1)\n",
        "    model = CrossAttnSeqClassifier(vocab, d_model, heads, max_in, max_out).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    for _ in range(steps):\n",
        "        kt, vt, y, q, out_len = make_batch(task, 128, L, vocab, M, K, Val)\n",
        "        kt, vt, y = kt.to(device), vt.to(device), y.to(device)\n",
        "        q = None if q is None else q.to(device)\n",
        "        logits, attn = model(kt, vt, out_len, q)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, vocab), y.reshape(-1)) if out_len > 1 else F.cross_entropy(logits[:, 0, :], y)\n",
        "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
        "    return model\n",
        "\n",
        "def plot_heatmap(attn_w, title):\n",
        "    a = attn_w[0].mean(dim=0).detach().cpu().numpy()\n",
        "    plt.figure(figsize=(6, 5)); plt.imshow(a, aspect='auto', interpolation='nearest'); plt.colorbar()\n",
        "    plt.title(title); plt.xlabel('input position (key/value index)'); plt.ylabel('output position (query index)')\n",
        "    plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ba88ae",
      "metadata": {},
      "source": [
        "## 1) Copy (expect diagonal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff61127a",
      "metadata": {},
      "outputs": [],
      "source": [
        "L, V = 16, 32\n",
        "copy_model = train('copy', steps=200, L=L, V=V)\n",
        "copy_model.eval()\n",
        "kt, vt, y, q, out_len = make_batch('copy', 1, L, V)\n",
        "with torch.no_grad():\n",
        "    logits, attn = copy_model(kt, vt, out_len, q)\n",
        "print('input :', kt[0].tolist())\n",
        "print('target:', y[0].tolist())\n",
        "print('pred  :', logits.argmax(dim=-1)[0].tolist())\n",
        "plot_heatmap(attn, 'Copy: mean over heads (expect diagonal)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eebcaf1",
      "metadata": {},
      "source": [
        "## 2) Reverse (expect anti-diagonal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4bf481",
      "metadata": {},
      "outputs": [],
      "source": [
        "reverse_model = train('reverse', steps=250, L=L, V=V)\n",
        "reverse_model.eval()\n",
        "kt, vt, y, q, out_len = make_batch('reverse', 1, L, V)\n",
        "with torch.no_grad():\n",
        "    logits, attn = reverse_model(kt, vt, out_len, q)\n",
        "print('input :', kt[0].tolist())\n",
        "print('target:', y[0].tolist())\n",
        "print('pred  :', logits.argmax(dim=-1)[0].tolist())\n",
        "plot_heatmap(attn, 'Reverse: mean over heads (expect anti-diagonal)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ee5e10",
      "metadata": {},
      "source": [
        "## 3) Pointer / lookup (expect sharp peak)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1609cf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "M, K, Val = 12, 12, 12\n",
        "ptr_model = train('ptr', steps=350, M=M, K=K, Val=Val)\n",
        "ptr_model.eval()\n",
        "kt, vt, y, q, out_len = make_batch('ptr', 1, L, 1 + K + Val, M=M, K=K, Val=Val)\n",
        "with torch.no_grad():\n",
        "    logits, attn = ptr_model(kt, vt, out_len, q)\n",
        "print('keys  :', kt[0].tolist())\n",
        "print('values:', vt[0].tolist())\n",
        "print('query :', int(q[0]))\n",
        "print('target:', int(y[0]))\n",
        "print('pred  :', int(logits[:, 0, :].argmax(dim=-1)[0]))\n",
        "plot_heatmap(attn, 'Pointer: attention over pair index (expect 1-hot-ish)')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
