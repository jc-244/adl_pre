{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial: PyTorch Code Walkthrough + a small Demo\n",
        "\n",
        "We’ll focus on the three practical takeaways in PyTorch’s nn.MultiheadAttention, then run a tiny demo to interpret attention maps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f287385c",
      "metadata": {},
      "source": [
        "## PyTorch `nn.MultiheadAttention` Implementation\n",
        "\n",
        "\n",
        "Source (pytorch/torch/nn/modules/activation.py):\n",
        "- https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/activation.py#L1089-L1566\n",
        "\n",
        "### Three engineering takeaways\n",
        "1) Supporting both input layouts: `(batch, seq, dim)` and `(seq, batch, dim)`.\n",
        "2) Supporting three masking controls: `key_padding_mask`, `attn_mask`, and `is_causal`.\n",
        "3) `need_weights` indirectly controls whether PyTorch uses the regular math implementation or CUDA-optimized SDPA kernels.\n",
        "\n",
        "**For a quick look at how these are implemented (in `F.multi_head_attention_forward`), see `f_mha_key.ipynb`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiheadAttention(Module):\n",
        "    # ...\n",
        "    __constants__ = [\"batch_first\"]\n",
        "    bias_k: torch.Tensor | None\n",
        "    bias_v: torch.Tensor | None\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=True,\n",
        "        add_bias_kv=False,\n",
        "        add_zero_attn=False,\n",
        "        kdim=None,\n",
        "        vdim=None,\n",
        "        batch_first=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        if embed_dim <= 0 or num_heads <= 0:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim and num_heads must be greater than 0,\"\n",
        "                f\" got embed_dim={embed_dim} and num_heads={num_heads} instead\"\n",
        "            )\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.batch_first = batch_first\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        if self.head_dim * num_heads != self.embed_dim:\n",
        "            raise AssertionError(\"embed_dim must be divisible by num_heads\")\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            # ...\n",
        "        else:\n",
        "            self.in_proj_weight = Parameter(\n",
        "                torch.empty((3 * embed_dim, embed_dim), **factory_kwargs)\n",
        "            )\n",
        "            self.register_parameter(\"q_proj_weight\", None)\n",
        "            self.register_parameter(\"k_proj_weight\", None)\n",
        "            self.register_parameter(\"v_proj_weight\", None)\n",
        "\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n",
        "        else:\n",
        "            self.register_parameter(\"in_proj_bias\", None)\n",
        "        self.out_proj = NonDynamicallyQuantizableLinear(\n",
        "            embed_dim, embed_dim, bias=bias, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self) -> None:\n",
        "        if self._qkv_same_embed_dim:\n",
        "            xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            # ...\n",
        "\n",
        "        if self.in_proj_bias is not None:\n",
        "            constant_(self.in_proj_bias, 0.0)\n",
        "            constant_(self.out_proj.bias, 0.0)\n",
        "        if self.bias_k is not None:\n",
        "            xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            xavier_normal_(self.bias_v)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: Tensor,\n",
        "        key: Tensor,\n",
        "        value: Tensor,\n",
        "        key_padding_mask: Tensor | None = None,\n",
        "        need_weights: bool = True,\n",
        "        attn_mask: Tensor | None = None,\n",
        "        average_attn_weights: bool = True,\n",
        "        is_causal: bool = False,\n",
        "    ) -> tuple[Tensor, Tensor | None]:\n",
        "        # ...\n",
        "        is_batched = query.dim() == 3\n",
        "\n",
        "        if self.batch_first and is_batched:\n",
        "            # ...\n",
        "\n",
        "        if not self._qkv_same_embed_dim:\n",
        "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                self.in_proj_weight,\n",
        "                self.in_proj_bias,\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=attn_mask,\n",
        "                use_separate_proj_weight=True,\n",
        "                q_proj_weight=self.q_proj_weight,\n",
        "                k_proj_weight=self.k_proj_weight,\n",
        "                v_proj_weight=self.v_proj_weight,\n",
        "                average_attn_weights=average_attn_weights,\n",
        "                is_causal=is_causal,\n",
        "            )\n",
        "        else:\n",
        "            attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                self.embed_dim,\n",
        "                self.num_heads,\n",
        "                self.in_proj_weight,\n",
        "                self.in_proj_bias,\n",
        "                self.bias_k,\n",
        "                self.bias_v,\n",
        "                self.add_zero_attn,\n",
        "                self.dropout,\n",
        "                self.out_proj.weight,\n",
        "                self.out_proj.bias,\n",
        "                training=self.training,\n",
        "                key_padding_mask=key_padding_mask,\n",
        "                need_weights=need_weights,\n",
        "                attn_mask=attn_mask,\n",
        "                average_attn_weights=average_attn_weights,\n",
        "                is_causal=is_causal,\n",
        "            )\n",
        "        if self.batch_first and is_batched:\n",
        "            return attn_output.transpose(1, 0), attn_output_weights\n",
        "        else:\n",
        "            return attn_output, attn_output_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31bf0f59",
      "metadata": {},
      "source": [
        "## A Demo Using `nn.MultiheadAttention`\n",
        "\n",
        "In this demo, we train a tiny cross-attention model on three toy tasks and visualize **attention weights**.\n",
        "\n",
        "**How to read the heatmap**\n",
        "- x-axis: input position (key/value index)\n",
        "- y-axis: output position (query index)\n",
        "- brighter = higher attention weight\n",
        "\n",
        "These synthetic tasks are designed so that “correct behavior” corresponds to a clear geometric pattern in the attention map.\n",
        "(We set `need_weights=True` for visualization.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42979c51",
      "metadata": {},
      "source": [
        "### Setup\n",
        "\n",
        "**What this model is doing (high level)**\n",
        "- Input: a sequence of tokens (used to form keys/values)\n",
        "- Queries: positions in the output sequence (or a single query token for the pointer task)\n",
        "- Output: predicted tokens, plus attention maps for inspection\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b38220b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "\n",
        "set_seed(0)\n",
        "torch.set_num_threads(min(4, os.cpu_count() or 1))\n",
        "device = torch.device('cpu')\n",
        "\n",
        "class CrossAttnSeqClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, max_in_len, max_out_len):\n",
        "        super().__init__()\n",
        "        self.vocab_size, self.max_in_len, self.max_out_len = vocab_size, max_in_len, max_out_len\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.in_pos_emb = nn.Embedding(max_in_len, d_model)\n",
        "        self.out_pos_emb = nn.Embedding(max_out_len, d_model)\n",
        "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=0.0, batch_first=True)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, key_tokens, value_tokens, out_len, query_token=None):\n",
        "        B, Tk = key_tokens.shape\n",
        "        if Tk > self.max_in_len or out_len > self.max_out_len:\n",
        "            raise ValueError('sequence too long for positional embeddings')\n",
        "        in_pos = torch.arange(Tk, device=key_tokens.device)\n",
        "        out_pos = torch.arange(out_len, device=key_tokens.device)\n",
        "        K = self.tok_emb(key_tokens) + self.in_pos_emb(in_pos)[None, :, :]\n",
        "        V = self.tok_emb(value_tokens) + self.in_pos_emb(in_pos)[None, :, :]\n",
        "        Q = self.out_pos_emb(out_pos)[None, :, :].expand(B, -1, -1)\n",
        "        if query_token is not None:\n",
        "            Q = Q + self.tok_emb(query_token)[:, None, :]\n",
        "        ctx, attn_w = self.attn(Q, K, V, need_weights=True, average_attn_weights=False)\n",
        "        return self.out(ctx), attn_w\n",
        "    \n",
        "def make_batch(task, B, L, V, M=12, K=12, Val=12):\n",
        "    if task != 'ptr':\n",
        "        x = torch.randint(1, V, (B, L), dtype=torch.long)\n",
        "        y = x if task == 'copy' else x.flip(1)\n",
        "        return x, x, y, None, L\n",
        "    keys = torch.randint(1, K + 1, (B, M), dtype=torch.long)\n",
        "    vals = torch.randint(K + 1, K + Val + 1, (B, M), dtype=torch.long)\n",
        "    idx = torch.randint(0, M, (B,), dtype=torch.long)\n",
        "    q = keys[torch.arange(B), idx]\n",
        "    y = vals[torch.arange(B), idx]\n",
        "    return keys, vals, y, q, 1\n",
        "\n",
        "def train(task, steps=250, L=16, V=32, M=12, K=12, Val=12, d_model=32, heads=4, lr=3e-3):\n",
        "    vocab = V if task != 'ptr' else 1 + K + Val\n",
        "    max_in, max_out = (L, L) if task != 'ptr' else (M, 1)\n",
        "    model = CrossAttnSeqClassifier(vocab, d_model, heads, max_in, max_out).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    for _ in range(steps):\n",
        "        kt, vt, y, q, out_len = make_batch(task, 128, L, vocab, M, K, Val)\n",
        "        kt, vt, y = kt.to(device), vt.to(device), y.to(device)\n",
        "        q = None if q is None else q.to(device)\n",
        "        logits, attn = model(kt, vt, out_len, q)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, vocab), y.reshape(-1)) if out_len > 1 else F.cross_entropy(logits[:, 0, :], y)\n",
        "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
        "    return model\n",
        "\n",
        "def plot_heatmap(attn_w, title):\n",
        "    a = attn_w[0].mean(dim=0).detach().cpu().numpy()\n",
        "    plt.figure(figsize=(6, 5)); plt.imshow(a, aspect='auto', interpolation='nearest'); plt.colorbar()\n",
        "    plt.title(title); plt.xlabel('input position (key/value index)'); plt.ylabel('output position (query index)')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ba88ae",
      "metadata": {},
      "source": [
        "### 1) Copy (expect diagonal)\n",
        "\n",
        "**Task:** reproduce the input sequence at the output (copy).\n",
        "\n",
        "**What to expect in attention:** a bright **diagonal**  \n",
        "Each output position should mainly attend to the same input position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff61127a",
      "metadata": {},
      "outputs": [],
      "source": [
        "L, V = 16, 32\n",
        "copy_model = train('copy', steps=200, L=L, V=V)\n",
        "copy_model.eval()\n",
        "kt, vt, y, q, out_len = make_batch('copy', 1, L, V)\n",
        "with torch.no_grad():\n",
        "    logits, attn = copy_model(kt, vt, out_len, q)\n",
        "print('input :', kt[0].tolist())\n",
        "print('target:', y[0].tolist())\n",
        "print('pred  :', logits.argmax(dim=-1)[0].tolist())\n",
        "plot_heatmap(attn, 'Copy: mean over heads (expect diagonal)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eebcaf1",
      "metadata": {},
      "source": [
        "### 2) Reverse (expect anti-diagonal)\n",
        "\n",
        "**Task:** output the input sequence in reverse order.\n",
        "\n",
        "**What to expect in attention:** a bright **anti-diagonal**  \n",
        "Output position *t* should attend to input position *(L−1−t)*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4bf481",
      "metadata": {},
      "outputs": [],
      "source": [
        "reverse_model = train('reverse', steps=250, L=L, V=V)\n",
        "reverse_model.eval()\n",
        "kt, vt, y, q, out_len = make_batch('reverse', 1, L, V)\n",
        "with torch.no_grad():\n",
        "    logits, attn = reverse_model(kt, vt, out_len, q)\n",
        "print('input :', kt[0].tolist())\n",
        "print('target:', y[0].tolist())\n",
        "print('pred  :', logits.argmax(dim=-1)[0].tolist())\n",
        "plot_heatmap(attn, 'Reverse: mean over heads (expect anti-diagonal)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ee5e10",
      "metadata": {},
      "source": [
        "### 3) Pointer / lookup (expect sharp peak)\n",
        "\n",
        "**Task:** given a set of (key, value) pairs, use the query token to “look up” the corresponding value.\n",
        "\n",
        "**What to expect in attention:** a **sharp peak** (almost one-hot)  \n",
        "The single output query should attend strongly to exactly one input slot (the matched key).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1609cf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "M, K, Val = 12, 12, 12\n",
        "ptr_model = train('ptr', steps=350, M=M, K=K, Val=Val)\n",
        "ptr_model.eval()\n",
        "kt, vt, y, q, out_len = make_batch('ptr', 1, L, 1 + K + Val, M=M, K=K, Val=Val)\n",
        "with torch.no_grad():\n",
        "    logits, attn = ptr_model(kt, vt, out_len, q)\n",
        "print('keys  :', kt[0].tolist())\n",
        "print('values:', vt[0].tolist())\n",
        "print('query :', int(q[0]))\n",
        "print('target:', int(y[0]))\n",
        "print('pred  :', int(logits[:, 0, :].argmax(dim=-1)[0]))\n",
        "plot_heatmap(attn, 'Pointer: attention over pair index (expect 1-hot-ish)')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
