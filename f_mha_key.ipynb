{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b2c3d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_head_attention_forward(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    embed_dim_to_check: int,\n",
        "    num_heads: int,\n",
        "    in_proj_weight: Optional[Tensor],\n",
        "    in_proj_bias: Optional[Tensor],\n",
        "    bias_k: Optional[Tensor],\n",
        "    bias_v: Optional[Tensor],\n",
        "    add_zero_attn: bool,\n",
        "    dropout_p: float,\n",
        "    out_proj_weight: Tensor,\n",
        "    out_proj_bias: Optional[Tensor],\n",
        "    training: bool = True,\n",
        "    key_padding_mask: Optional[Tensor] = None,\n",
        "    need_weights: bool = True,\n",
        "    attn_mask: Optional[Tensor] = None,\n",
        "    use_separate_proj_weight: bool = False,\n",
        "    q_proj_weight: Optional[Tensor] = None,\n",
        "    k_proj_weight: Optional[Tensor] = None,\n",
        "    v_proj_weight: Optional[Tensor] = None,\n",
        "    static_k: Optional[Tensor] = None,\n",
        "    static_v: Optional[Tensor] = None,\n",
        "    average_attn_weights: bool = True,\n",
        "    is_causal: bool = False,\n",
        ") -> tuple[Tensor, Optional[Tensor]]:\n",
        "    # ...\n",
        "\n",
        "    # ...\n",
        "\n",
        "    is_batched = _mha_shape_check(\n",
        "        query, key, value, key_padding_mask, attn_mask, num_heads\n",
        "    )\n",
        "\n",
        "    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n",
        "    # is batched, run the computation and before returning squeeze the\n",
        "    # batch dimension so that the output doesn't carry this temporary batch dimension.\n",
        "    if not is_batched:\n",
        "        # unsqueeze if the input is unbatched\n",
        "        query = query.unsqueeze(1)\n",
        "        key = key.unsqueeze(1)\n",
        "        value = value.unsqueeze(1)\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(0)\n",
        "\n",
        "    # set up shape vars\n",
        "    tgt_len, bsz, embed_dim = query.shape\n",
        "    src_len, _, _ = key.shape\n",
        "\n",
        "    key_padding_mask = _canonical_mask(\n",
        "        mask=key_padding_mask,\n",
        "        mask_name=\"key_padding_mask\",\n",
        "        other_type=_none_or_dtype(attn_mask),\n",
        "        other_name=\"attn_mask\",\n",
        "        target_type=query.dtype,\n",
        "    )\n",
        "\n",
        "    if is_causal and attn_mask is None:\n",
        "        raise RuntimeError(\n",
        "            \"Need attn_mask if specifying the is_causal hint. \"\n",
        "            \"You may use the Transformer module method \"\n",
        "            \"`generate_square_subsequent_mask` to create this mask.\"\n",
        "        )\n",
        "\n",
        "    if is_causal and key_padding_mask is None and not need_weights:\n",
        "        # when we have a kpm or need weights, we need attn_mask\n",
        "        # Otherwise, we use the is_causal hint go as is_causal\n",
        "        # indicator to SDPA.\n",
        "        attn_mask = None\n",
        "    else:\n",
        "        attn_mask = _canonical_mask(\n",
        "            mask=attn_mask,\n",
        "            mask_name=\"attn_mask\",\n",
        "            other_type=None,\n",
        "            other_name=\"\",\n",
        "            target_type=query.dtype,\n",
        "            check_other=False,\n",
        "        )\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # We have the attn_mask, and use that to merge kpm into it.\n",
        "            # Turn off use of is_causal hint, as the merged mask is no\n",
        "            # longer causal.\n",
        "            is_causal = False\n",
        "\n",
        "    # ...\n",
        "\n",
        "    if isinstance(embed_dim, torch.Tensor):\n",
        "        # embed_dim can be a tensor when JIT tracing\n",
        "        head_dim = embed_dim.div(num_heads, rounding_mode=\"trunc\")\n",
        "    else:\n",
        "        head_dim = embed_dim // num_heads\n",
        "\n",
        "    # ...\n",
        "\n",
        "    #\n",
        "    # compute in-projection\n",
        "    #\n",
        "    if not use_separate_proj_weight:\n",
        "        # ...\n",
        "        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
        "    else:\n",
        "        # ...\n",
        "        if in_proj_bias is None:\n",
        "            b_q = b_k = b_v = None\n",
        "        else:\n",
        "            b_q, b_k, b_v = in_proj_bias.chunk(3)\n",
        "        q, k, v = _in_projection(\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            q_proj_weight,\n",
        "            k_proj_weight,\n",
        "            v_proj_weight,\n",
        "            b_q,\n",
        "            b_k,\n",
        "            b_v,\n",
        "        )\n",
        "\n",
        "    # prep attention mask\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        # ensure attn_mask's dim is 3\n",
        "        if attn_mask.dim() == 2:\n",
        "            correct_2d_size = (tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_2d_size:\n",
        "                raise RuntimeError(\n",
        "                    f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\"\n",
        "                )\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "        elif attn_mask.dim() == 3:\n",
        "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_3d_size:\n",
        "                raise RuntimeError(\n",
        "                    f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\"\n",
        "                )\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n",
        "            )\n",
        "\n",
        "    # ...\n",
        "\n",
        "    #\n",
        "    # reshape q, k, v for multihead attention and make them batch first\n",
        "    #\n",
        "    # pyrefly: ignore [no-matching-overload]\n",
        "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    # pyrefly: ignore [no-matching-overload]\n",
        "    k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    # pyrefly: ignore [no-matching-overload]\n",
        "    v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
        "\n",
        "    # ...\n",
        "\n",
        "    # update source sequence length after adjustments\n",
        "    src_len = k.size(1)\n",
        "\n",
        "    # merge key padding and attention masks\n",
        "    if key_padding_mask is not None:\n",
        "        # ...\n",
        "\n",
        "        key_padding_mask = (\n",
        "            key_padding_mask.view(bsz, 1, 1, src_len)\n",
        "            .expand(-1, num_heads, -1, -1)\n",
        "            .reshape(bsz * num_heads, 1, src_len)\n",
        "        )\n",
        "        if attn_mask is None:\n",
        "            attn_mask = key_padding_mask\n",
        "        else:\n",
        "            attn_mask = attn_mask + key_padding_mask\n",
        "\n",
        "    # adjust dropout probability\n",
        "    if not training:\n",
        "        dropout_p = 0.0\n",
        "\n",
        "    #\n",
        "    # (deep breath) calculate attention and out projection\n",
        "    #\n",
        "\n",
        "    if need_weights:\n",
        "        _B, _Nt, E = q.shape\n",
        "        q_scaled = q * math.sqrt(1.0 / float(E))\n",
        "\n",
        "        if is_causal and attn_mask is None:\n",
        "            raise AssertionError(\"FIXME: is_causal not implemented for need_weights\")\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_output_weights = torch.baddbmm(\n",
        "                attn_mask, q_scaled, k.transpose(-2, -1)\n",
        "            )\n",
        "        else:\n",
        "            attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
        "        attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
        "        if dropout_p > 0.0:\n",
        "            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n",
        "\n",
        "        attn_output = torch.bmm(attn_output_weights, v)\n",
        "\n",
        "        attn_output = (\n",
        "            # pyrefly: ignore [no-matching-overload]\n",
        "            attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
        "        )\n",
        "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "\n",
        "        # optionally average attention weights over heads\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        if average_attn_weights:\n",
        "            attn_output_weights = attn_output_weights.mean(dim=1)\n",
        "\n",
        "        if not is_batched:\n",
        "            # squeeze the output if input was unbatched\n",
        "            attn_output = attn_output.squeeze(1)\n",
        "            attn_output_weights = attn_output_weights.squeeze(0)\n",
        "        return attn_output, attn_output_weights\n",
        "    else:\n",
        "        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
        "        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
        "        # in order to match the input for SDPA of (N, num_heads, L, S)\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
        "                attn_mask = attn_mask.unsqueeze(0)\n",
        "            else:\n",
        "                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
        "\n",
        "        q = q.view(bsz, num_heads, tgt_len, head_dim)\n",
        "        # pyrefly: ignore [no-matching-overload]\n",
        "        k = k.view(bsz, num_heads, src_len, head_dim)\n",
        "        # pyrefly: ignore [no-matching-overload]\n",
        "        v = v.view(bsz, num_heads, src_len, head_dim)\n",
        "\n",
        "        attn_output = scaled_dot_product_attention(\n",
        "            q, k, v, attn_mask, dropout_p, is_causal\n",
        "        )\n",
        "        attn_output = (\n",
        "            # pyrefly: ignore [no-matching-overload]\n",
        "            attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
        "        )\n",
        "\n",
        "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "        if not is_batched:\n",
        "            # squeeze the output if input was unbatched\n",
        "            attn_output = attn_output.squeeze(1)\n",
        "        return attn_output, None\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
